{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    },
    "colab": {
      "name": "AMMI_EXPRL_Exercise.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "_UWM-gGi76pF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!git clone https://github.com/rlgammazero/mvarl_hands_on.git > /dev/null 2>&1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J5EbzJ1A78Bk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import sys\n",
        "sys.path.insert(0, './mvarl_hands_on/utils')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KDPYhYvR7yKZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import cvxpy as cp\n",
        "import matplotlib.pyplot as plt\n",
        "import gym\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S5twn7Qv7yKf",
        "colab_type": "text"
      },
      "source": [
        "# Finite-Horizon MDPs\n",
        "We consider finite horizon problems with horizon $H$. For simplicity, we consider MDPs with stationary transitions and rewards, ie these functions do not depend on the stage ($p_h =p$, $r_h=r$ for any $h \\in [H]$).\n",
        "\n",
        "The value of a policy or the optimal value function can be computed using *backward induction*.\n",
        "\n",
        "\n",
        "Given a deterministic (non-stationary) policy $\\pi = (\\pi_1, \\pi_2, \\ldots, \\pi_H)$, backward induction applies the Bellman operator defined as\n",
        "$$\n",
        "V_h^\\pi(s) = \\sum_{s'} p(s'|s,\\pi_h(s)) \\left( r(s,\\pi_h(s),s') + V_{h+1}^\\pi(s')\\right)\n",
        "$$\n",
        "where $V_{H+1}(s) = 0$, for any $s$. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7l8433mz7yKh",
        "colab_type": "code",
        "outputId": "0fa37e32-63e0-4f8e-8b48-6904913935df",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 251
        }
      },
      "source": [
        "env = RiverSwim(6)\n",
        "H = 10\n",
        "print(\"Reward matrix: \", env.R.shape)\n",
        "print(env.R)\n",
        "print()\n",
        "print(\"Transition matrix: \", env.P.shape)\n",
        "print(\"Transitions probabilities for state s_1:\")\n",
        "print(env.P[1])\n",
        "\n",
        "print(env.P[0, 1, 1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reward matrix:  (6, 2)\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "\n",
            "Transition matrix:  (6, 2, 6)\n",
            "Transitions probabilities for state s_1:\n",
            "[[1.   0.   0.   0.   0.   0.  ]\n",
            " [0.05 0.6  0.35 0.   0.   0.  ]]\n",
            "0.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiXHdYle7yKm",
        "colab_type": "text"
      },
      "source": [
        "# Backward induction (aka Value Iteration)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_KZXn8A7yKo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def backward_induction(P, R, H):\n",
        "    \"\"\"\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "\n",
        "        Returns:\n",
        "            The optimal V-function: array of shape (horizon, S)\n",
        "            The optimal policy\n",
        "        \n",
        "        V[h, s]\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^*(h, s) using the Bellman optimality equation:\n",
        "\n",
        "            V[h, s] = max_a  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            for a in range(A):\n",
        "                tmp = R[s, a] + np.dot(P[s, a],  V[h + 1])\n",
        "                if (a == 0) or (tmp > V[h, s]):\n",
        "                    policy[h, s] = a\n",
        "                    V[h, s] = tmp\n",
        "    return V, policy\n",
        "\n",
        "def policy_evaluation(P, R, H, policy):\n",
        "    \"\"\"\n",
        "    policy[h, s] = action to take in s at time/stage h \n",
        "\n",
        "        Parameters:\n",
        "            P: transition function (S,A,S)-dim matrix\n",
        "            R: reward function (S,A)-dim matrix\n",
        "            H: horizon\n",
        "            policy: policy (H,S)-dim matrix\n",
        "\n",
        "        Returns:\n",
        "            The V-function of the given policy\n",
        "    \"\"\"\n",
        "    S, A = P.shape[0], P.shape[1]\n",
        "    V = np.zeros((H + 1, S))\n",
        "    for h in reversed(range(H)):\n",
        "        for s in range(S):\n",
        "            \"\"\" \n",
        "            Here, we compute V^pi(h, s) using the Bellman equation for the policy pi:\n",
        "\n",
        "            a = policy[h,s]\n",
        "            V[h, s] =  R[s, a] + sum_{s'} P[s, a, s']*V[h+1, s']\n",
        "            \"\"\"\n",
        "            a = policy[h,s]\n",
        "            V[h, s] = R[s, a] + (P[s, a, :]).dot(V[h+1, :])\n",
        "    return V"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pvmhRCN07yKs",
        "colab_type": "text"
      },
      "source": [
        "Compute solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vVjkydge7yKt",
        "colab_type": "code",
        "outputId": "20ed5b5f-b0be-4710-ebb1-de02a8db0faa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 665
        }
      },
      "source": [
        "Vstar, POLstar = backward_induction(env.P, env.R, H)\n",
        "\n",
        "print(\"Optimal policy\")\n",
        "print(np.round(Vstar))\n",
        "\n",
        "print(POLstar)\n",
        "\n",
        "# To test your implementation:\n",
        "V_policy = policy_evaluation(env.P, env.R, H, POLstar)\n",
        "# V_policy must be equal to Vstar\n",
        "print(\"difference V_policy - Vstar\")\n",
        "print(np.abs(V_policy - Vstar).sum())\n",
        "print(\".....\")\n",
        "print(np.round(V_policy))\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal policy\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n",
            "[[1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [1 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 1 1 1 1 1]\n",
            " [0 0 1 1 1 1]\n",
            " [0 0 0 1 1 1]\n",
            " [0 0 0 0 1 1]\n",
            " [0 0 0 0 0 1]]\n",
            "difference V_policy - Vstar\n",
            "0.0\n",
            ".....\n",
            "[[0. 1. 1. 2. 4. 5.]\n",
            " [0. 0. 1. 2. 3. 5.]\n",
            " [0. 0. 1. 2. 3. 4.]\n",
            " [0. 0. 1. 1. 3. 4.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 1. 2. 3.]\n",
            " [0. 0. 0. 0. 1. 3.]\n",
            " [0. 0. 0. 0. 1. 2.]\n",
            " [0. 0. 0. 0. 0. 2.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 0. 0. 0. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ByvQboy7XsA",
        "colab_type": "code",
        "outputId": "5e456e0b-73d2-4f99-fe1f-fb6f6c9786a2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719
        }
      },
      "source": [
        "S, A = env.R.shape   # number of states and actions\n",
        "Phat = np.zeros((S, A, S))\n",
        "Rhat = np.zeros((S, A))\n",
        "\n",
        "N_sa = np.zeros((S, A)) # number of visits to each state-action\n",
        "N_sas = np.zeros((S, A, S))\n",
        "S_sa = np.zeros((S, A)) # sum of rewards obtained when visiting each\n",
        "                        # state, action\n",
        "\n",
        "# Interact with the environment to estimate P and R\n",
        "\n",
        "nb_episodes = 200\n",
        "# Loop over episodes\n",
        "for ep in range(nb_episodes):\n",
        "    state = env.reset()\n",
        "    for h in range(H):\n",
        "        action = np.random.choice(A)\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "\n",
        "        # Update estimates\n",
        "        N_sa[state, action] += 1\n",
        "        N_sas[state, action, next_state] +=1\n",
        "        S_sa[state, action] += reward\n",
        "        \n",
        "        Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "        Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "\n",
        "        state = next_state \n",
        "\n",
        "# Check confidence interval for rewards\n",
        "#  - With high probability, we have\n",
        "#    R(s, a) is in the interval\n",
        "#            [R_hat[s,a] - beta_r[s,a], R_hat[s,a] + beta_r[s,a]]\n",
        "#\n",
        "#  where beta_r[s, a] = sqrt(log(S*A*N[s,a])/N[s,a] )\n",
        "# \n",
        "beta_r = np.zeros((S, A))\n",
        "beta_p = np.zeros((S, A))\n",
        "for s in range(S):\n",
        "    for a in range(A):\n",
        "        n = max(N_sa[s,a], 1)\n",
        "        beta_r[s, a] = np.sqrt(np.log(S*A*n)/n )\n",
        "        beta_p[s, a] = np.sqrt(S*np.log(S*A*n)/n )\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(Rhat)\n",
        "print(Rhat + beta_r)\n",
        "print(Rhat - beta_r)\n",
        "print(\"True R\")\n",
        "print(env.R)\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "  ||P(. | s,a) - Phat(.| s,a)||_1 <= beta_p(s,a)\n",
        "\"\"\"\n",
        "print(\"chechink probabilities\")\n",
        "for s in range(S):\n",
        "    for a in range(A):\n",
        "        norm_1 = np.abs(env.P[s, a, :] - Phat[s,a, :]).sum()\n",
        "        print(norm_1, beta_p[s,a])\n",
        "        assert( norm_1 <= beta_p[s,a])\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]]\n",
            "[[0.12282366 0.11758106]\n",
            " [0.17387579 0.17219099]\n",
            " [0.30644858 0.31399734]\n",
            " [0.66625152 0.64354573]\n",
            " [1.26056611 1.26056611]\n",
            " [1.57635867 1.57635867]]\n",
            "[[-0.11282366 -0.11758106]\n",
            " [-0.17387579 -0.17219099]\n",
            " [-0.30644858 -0.31399734]\n",
            " [-0.66625152 -0.64354573]\n",
            " [-1.26056611 -1.26056611]\n",
            " [-1.57635867 -1.57635867]]\n",
            "True R\n",
            "[[0.005 0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    0.   ]\n",
            " [0.    1.   ]]\n",
            "chechink probabilities\n",
            "0.0 0.2886078367094033\n",
            "0.036419753086419704 0.2880135887055967\n",
            "0.0 0.4259069716482842\n",
            "0.04249084249084241 0.4217800698966739\n",
            "0.0 0.7506426562621777\n",
            "0.18235294117647055 0.7691332605581642\n",
            "0.0 1.6319762569439942\n",
            "0.1333333333333333 1.5763586678760644\n",
            "0.0 3.0877437541097605\n",
            "0.8 3.0877437541097605\n",
            "1.0 3.8612743879097744\n",
            "1.0 3.8612743879097744\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1nCGWuw7yKy",
        "colab_type": "text"
      },
      "source": [
        "## UCRL"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zoWtsh7v7yK0",
        "colab_type": "text"
      },
      "source": [
        "UCRL is an algorithm for efficient exploration in finite-horizon tabular MDP.\n",
        "In this setting, the regret is defined as\n",
        "$$R(K) = \\sum_{k=1}^K V^\\star_1(s_{k,1}) - V^{\\pi_k}_1(s_{k,1})$$\n",
        "UCBVI enjoys a regret bound of order $O(\\sqrt{HSAK})$.\n",
        "\n",
        "The structure of the algorithm is as follow\n",
        "\n",
        "For $k = 1, \\ldots, K$ do<br>\n",
        "> Solve optimistic planning problem -> $(V_k, Q_k, \\pi_k)$<br>\n",
        "> Execute the optimistic policy $\\pi_k$ for $H$ steps<br>\n",
        ">> for $h=1, \\ldots, H$<br>\n",
        ">>> $a_{k,h} = \\pi(s_{k,h})$<br>\n",
        ">>> execute $a_{k,h}$, observe $r_{k,h}$ and $s_{k, h+1}$<br>\n",
        ">>> $N(s_{k,h}, a_{k,h}, s_{k,h+1}) += 1$ (update also estimated reward and transitions)\n",
        "\n",
        "<font color='#ed7d31'>Optimistic planning</font>\n",
        "At each episode, UCRL computes the optimal solution by solving the following \"extended\" problem\n",
        "$$\n",
        "V_h^\\star(s) =  \\max_{r \\in B_r(s,a)} r + \\max_{p \\in B_p(s,a)} \\sum_{s'} p(s') V_{h+1}(s') \n",
        "$$\n",
        "where $V_{H+1}(s) = 0$ and $B_r(s,a)$ and $B_p(s,a)$ are confidence intervals on the estimated transitions and rewards:\n",
        "\n",
        "$$\n",
        "B_r(s,a) = \\{ r(s,a):  |r_s, a) - \\hat{r}(s,a)| \\leq \\beta_r(s,a)  \\}\n",
        "$$\n",
        "\n",
        "$$\n",
        "B_p(s,a) = \\{ p(\\cdot|s,a):  ||p(\\cdot|s,a) - \\hat{p}(\\cdot|s,a)||_1 \\leq \\beta_p(s,a)  \\}\n",
        "$$\n",
        "where \n",
        "\n",
        "$$\n",
        "\\beta_r(s, a) = \\sqrt{ \\frac{ \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  } \\\\\n",
        "\\beta_p(s, a) = \\sqrt{ \\frac{  S \\log(S A N^+(s,a) / \\delta)}{ N^+(s, a)}  }\n",
        "$$\n",
        "and where  $N^+(s, a) = \\max(1, N(s, a))$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ji7IShLN7yK1",
        "colab_type": "text"
      },
      "source": [
        "---\n",
        "The following function computes:\n",
        "$$\\max_{x \\in B_p} \\sum_{s'} x(s') V(s') $$\n",
        "where $B_p = [P-\\beta, P+\\beta]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "57zVp32L7yK3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def LPprobaH(v, P, beta, verbose=0):\n",
        "    \"\"\"\n",
        "        max_x v^T x\n",
        "        s.t.    0 <= x_i <= 1\n",
        "                \\sum_i |x_i - p_i| \\leq beta\n",
        "                \\sum_i x_i = 1\n",
        "    \"\"\"\n",
        "    sorted_idxs = np.argsort(v)[::-1]\n",
        "\n",
        "    pest = P.copy()\n",
        "    idx = sorted_idxs[0]\n",
        "    pest[idx] = min(1., P[idx] + beta / 2.)\n",
        "    delta = pest.sum()\n",
        "    j = len(P)-1\n",
        "    while delta > 1:\n",
        "        idx_j = sorted_idxs[j]\n",
        "        m = max(0, 1. - delta + pest[idx_j])\n",
        "        delta = delta - pest[idx_j] + m\n",
        "        pest[idx_j] = m\n",
        "        j -= 1\n",
        "    w = np.dot(pest, v)\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pk0v7jg67yK7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def UCRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    policy = np.zeros((H, S), dtype=np.int)\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    V = np.zeros((H + 1, S))\n",
        "\n",
        "    S_sa = np.zeros((S,A))\n",
        "    \n",
        "    delta = 0.5\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute optimistic solution\n",
        "        # 1. compute confidence intervals\n",
        "        N = np.maximum(N_sa, 1)\n",
        "        LOGT = np.log(S * A * N / delta)\n",
        "        beta_r = np.sqrt( LOGT/N )\n",
        "        beta_p = np.sqrt( S*LOGT/N )\n",
        "        \n",
        "        # 2. run extended value iteration\n",
        "        V.fill(0)\n",
        "        for h in reversed(range(H)):\n",
        "            for s in range(S):\n",
        "                temp = np.zeros(A)\n",
        "                for a in range(A):\n",
        "                    dotp = LPprobaH(V[h + 1], Phat[s, a], beta_p[s, a])\n",
        "                    temp[a] = Rhat[s, a] + beta_r[s,a] + dotp\n",
        "                V[h, s] = temp.max() \n",
        "                policy[h, s] = temp.argmax()\n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Phat, Rhat, N_sa, N_sas)\n",
        "            N_sa[state, action] += 1\n",
        "            N_sas[state, action, next_state] +=1\n",
        "            S_sa[state, action] += reward\n",
        "            \n",
        "            Rhat[state, action] = S_sa[state, action]/N_sa[state, action]\n",
        "            Phat[state, action, :] = N_sas[state, action, :]/N_sa[state, action]\n",
        "\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-UTr3Gq7yK_",
        "colab_type": "code",
        "outputId": "91262121-9765-4a6d-a824-90fdbcab9875",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = UCRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running simulation: 0\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.03363150300000001\n",
            "regret[100]: 0.03363150300000001\n",
            "regret[150]: 0.031235826999999994\n",
            "regret[200]: 0.04283525331249999\n",
            "regret[250]: 0.031235826999999994\n",
            "regret[300]: 0.04283525331249999\n",
            "regret[350]: 0.03363150300000001\n",
            "regret[400]: 0.03363150300000001\n",
            "regret[450]: 0.031235826999999994\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.031235826999999994\n",
            "regret[600]: 0.03363150300000001\n",
            "regret[650]: 0.03363150300000001\n",
            "regret[700]: 0.047646061750000024\n",
            "Running simulation: 1\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3427612076249999\n",
            "regret[100]: 0.03363150300000001\n",
            "regret[150]: 0.08074587581249998\n",
            "regret[200]: 0.03363150300000001\n",
            "regret[250]: 0.03363150300000001\n",
            "regret[300]: 0.03363150300000001\n",
            "regret[350]: 0.031235826999999994\n",
            "regret[400]: 0.09537021674999996\n",
            "regret[450]: 0.031235826999999994\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.035121323718749986\n",
            "regret[700]: 0.09686003746874994\n",
            "Running simulation: 2\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.34572321562499986\n",
            "regret[100]: 0.039255522531249976\n",
            "regret[150]: 0.03306478075000002\n",
            "regret[200]: 0.129303182\n",
            "regret[250]: 0.07835019981249997\n",
            "regret[300]: 0.03363150300000001\n",
            "regret[350]: 0.031235826999999994\n",
            "regret[400]: 0.03363150300000001\n",
            "regret[450]: 0.09297454075\n",
            "regret[500]: 0.031235826999999994\n",
            "regret[550]: 0.03363150300000001\n",
            "regret[600]: 0.031235826999999994\n",
            "regret[650]: 0.05004173775000004\n",
            "regret[700]: 0.031235826999999994\n",
            "Running simulation: 3\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.3451564933749999\n",
            "regret[100]: 0.06489815099999996\n",
            "regret[150]: 0.07835019981249997\n",
            "regret[200]: 0.03363150300000001\n",
            "regret[250]: 0.03363150300000001\n",
            "regret[300]: 0.031235826999999994\n",
            "regret[350]: 0.03363150300000001\n",
            "regret[400]: 0.03363150300000001\n",
            "regret[450]: 0.03363150300000001\n",
            "regret[500]: 0.03363150300000001\n",
            "regret[550]: 0.09537021674999996\n",
            "regret[600]: 0.035121323718749986\n",
            "regret[650]: 0.03272564771874997\n",
            "regret[700]: 0.031235826999999994\n",
            "Running simulation: 4\n",
            "regret[0]: 0.3023839779999999\n",
            "regret[50]: 0.34572321562499986\n",
            "regret[100]: 0.03363150300000001\n",
            "regret[150]: 0.03306478075000002\n",
            "regret[200]: 0.03363150300000001\n",
            "regret[250]: 0.031235826999999994\n",
            "regret[300]: 0.10410031656249999\n",
            "regret[350]: 0.03363150300000001\n",
            "regret[400]: 0.031235826999999994\n",
            "regret[450]: 0.1741602290312499\n",
            "regret[500]: 0.035121323718749986\n",
            "regret[550]: 0.09686003746874994\n",
            "regret[600]: 0.047646061750000024\n",
            "regret[650]: 0.035121323718749986\n",
            "regret[700]: 0.031235826999999994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M49yfZ_S7yLE",
        "colab_type": "code",
        "outputId": "2459e14d-6d74-410a-92e0-f2288f4a408e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        }
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "ucrl_regret = mean_regret"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX4AAAD4CAYAAADrRI2NAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nJ7MXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "myPuG9Er7yLL",
        "colab_type": "text"
      },
      "source": [
        "# Posterior Sampling for RL\n",
        "\n",
        "At each iteration, PSRL samples one MDP from the posterior distribution and run the associated optimal policy."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9gxP6B9r7yLN",
        "colab_type": "text"
      },
      "source": [
        "Implement posterior sampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cl9YwtfJ7yLP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def PSRL(mdp, H, nb_episodes, VSTAR=0):\n",
        "    reward_prior = [1,1]\n",
        "    S, A = mdp.Ns, mdp.Na\n",
        "    Phat = np.ones((S,A,S)) / S\n",
        "    Rhat = np.zeros((S,A))\n",
        "    N_sas = np.zeros((S,A,S), dtype=np.int)\n",
        "    N_sa = np.zeros((S,A), dtype=np.int)\n",
        "    regret = np.zeros((nb_episodes,))\n",
        "    \n",
        "    for k in range(nb_episodes):\n",
        "        \n",
        "        # compute policy\n",
        "        # 1. sample MDP\n",
        "        R = np.zeros_like(Rhat)\n",
        "        P = np.zeros((S, A, S))\n",
        "        for s in range(S):\n",
        "            for a in range(A):\n",
        "                # sample transition matrix\n",
        "                # P[s, a] follows a dirichlet Dirichlet distribution of parameters N_sas[s, a,:] + 1\n",
        "                P[s, a] = ...\n",
        "\n",
        "                # posterior for Bernoulli rewards\n",
        "                N = N_sa[s, a]\n",
        "                v = N * Rhat[s, a]\n",
        "                a0 = reward_prior[0] + v\n",
        "                b0 = reward_prior[1] + N - v\n",
        "                p = np.random.beta(a=a0, b=b0, size=1).item()\n",
        "                R[s, a] = p\n",
        "        \n",
        "        # 2. compute optimal policy\n",
        "        V, policy = ...\n",
        "        \n",
        "        # execute policy\n",
        "        initial_state = state = mdp.reset()\n",
        "        for h in range(H):\n",
        "            action = policy[h][state]\n",
        "            next_state, reward, done, _ = mdp.step(action)\n",
        "            \n",
        "            # update estimates (Rhat, N_sa, N_sas)\n",
        "            ...\n",
        "            \n",
        "            state = next_state\n",
        "        \n",
        "        # update regret\n",
        "        Vpi = policy_evaluation(mdp.P, mdp.R, H, policy)\n",
        "        regret[k] = VSTAR[0][initial_state] - Vpi[0][initial_state]\n",
        "        \n",
        "        if k % 50 == 0:\n",
        "            print(\"regret[{}]: {}\".format(k, regret[k]))\n",
        "\n",
        "    return regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zT38aVOB7yLU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nb_repetitions = 5\n",
        "nb_episodes = 750\n",
        "regrets = np.zeros((nb_repetitions, nb_episodes))\n",
        "for it in range(nb_repetitions):\n",
        "    print(\"Running simulation: {}\".format(it))\n",
        "    regrets[it] = PSRL(mdp=env, H=H, nb_episodes=nb_episodes, VSTAR=Vstar)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQhP7cpn7yLY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = regrets.cumsum(axis=-1)\n",
        "mean_regret = x.mean(axis=0)\n",
        "std_regret = x.std(axis=0)\n",
        "plt.plot(mean_regret)\n",
        "plt.fill_between(np.arange(nb_episodes), mean_regret - std_regret, mean_regret + std_regret, alpha=0.1)\n",
        "plt.ylabel('regret')\n",
        "\n",
        "# SAVE PSRL REGRET\n",
        "psrl_regret = mean_regret"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "phHZWwdJ7yLc",
        "colab_type": "text"
      },
      "source": [
        "Compare algorithms"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvcqwaJY7yLd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10,8))\n",
        "plt.plot(ucrl_regret, label='UCRL-H')\n",
        "plt.plot(psrl_regret, label='PSRL')\n",
        "plt.legend()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "it2tQtxL7yLi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}