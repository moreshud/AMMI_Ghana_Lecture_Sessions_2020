{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Global Optimization with Gaussian Processes [Bayesian Optimization]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adapted from \n",
    "#### Gaussian Process Summer School 2018\n",
    "\n",
    "_Author_ Sanket Kamthe and Marc Deisenroth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of this tutorial session is to illustrate how to use Gaussian processes for global optimization. \n",
    "\n",
    "We will focus on two aspects of Bayesian optimization (BO): \n",
    "1. Choice of the model \n",
    "2. Acquisition function.\n",
    "\n",
    "The technical material associated to the methods used in this lab can be found in the lecture slides. We have tried to use the same notation as the one used in lecture slides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'GPy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9f8405930965>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mGPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgridspec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstyle\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstyle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'GPy'"
     ]
    }
   ],
   "source": [
    "import GPy\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from matplotlib import gridspec\n",
    "import matplotlib.style as style\n",
    "%matplotlib inline\n",
    "style.use('ggplot')\n",
    "\n",
    "# If colour blind uncomment line below\n",
    "style.use('seaborn-colorblind')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before starting with the lab, remember that (BO) is an heuristic for global optimization of black-box functions. Let $f: {\\mathcal X} \\to R$ be a 'well behaved' continuous function defined on a compact subset ${\\mathcal X} \\subseteq R^d$. Our goal is to solve the global optimization problem of finding\n",
    "$$ x_{M} = \\arg \\min_{x \\in {\\mathcal X}} f(x). $$\n",
    "\n",
    "We assume that $f$ is a *black-box* from which only perturbed evaluations of the type $y_i = f(x_i) + \\epsilon_i$, with $\\epsilon_i \\sim\\mathcal{N}(0,\\sigma^2)$, are  available. The goal is to find $x_M$ by minimizing the number of evaluations of $f$. To do this, we need to determine two crucial bits:\n",
    "\n",
    "1. A **Gaussian process** that will capture the our beliefs on $f$. \n",
    "\n",
    "2. An **acquisition function** that based on the model will be useful to determine where to collect new evaluations of f. \n",
    "\n",
    "Remember that every time a new data point is collected the model is updated and the acquisition function optimized again. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create some benchmark functions to work with. We only work with 1 D functions as they are easy to plot and visualise. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Objective:\n",
    "    def __init__(self, func=None, limits=[0.0, 1.0], true_min=0.0, noise_std=0.0):\n",
    "        self.noise_std = noise_std\n",
    "        self.limits = limits\n",
    "        self.f_true = func\n",
    "        self.true_min = true_min\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.f_true(x) + np.random.randn(*x.shape) * self.noise_std\n",
    "    \n",
    "    @classmethod\n",
    "    def forrester(cls):\n",
    "        \n",
    "        '''\n",
    "        Details at \n",
    "        https://www.sfu.ca/~ssurjano/forretal08.html\n",
    "        \n",
    "        '''\n",
    "        def forrester(x):\n",
    "            return (6.0 * x - 2 ) ** 2 * np.sin(12 * x - 4)\n",
    "        \n",
    "        return cls(func=forrester, limits = [0.0, 1.0], true_min=0.78 )\n",
    "    \n",
    "    @classmethod\n",
    "    def rastrigrin(cls):\n",
    "        \"\"\"\n",
    "        https://www.sfu.ca/~ssurjano/rastr.html\n",
    "        \n",
    "        \"\"\"\n",
    "        def rastgrin(x):\n",
    "            return (6.0 * x - 2 ) ** 2 * np.sin(12 * x - 4)\n",
    "        \n",
    "        return cls(func=rastrigrin, limits = [-5.12, 5.12], true_min=0.0)\n",
    "    \n",
    "    @classmethod\n",
    "    def humps(cls):\n",
    "        '''\n",
    "        Custom function that shows importance of exploration\n",
    "        '''\n",
    "        def humps(x):\n",
    "            return - (np.exp(-(x - 2) ** 2) + np.exp(-(x - 6) ** 2 / 10) + 1/ (x ** 2 + 1))\n",
    "        \n",
    "        return cls(func=humps, limits = [-2, 10], true_min=2.0, noise_std=0.00 )\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running example\n",
    "\n",
    "We start with a one-dimensional example. Consider here the Forrester function \n",
    "\n",
    "$$f(x) =(6x-2)^2 \\sin(12x-4),$$ defined on the interval $[0, 1]$. \n",
    "\n",
    "The minimum of this function is located at $x_{min}=0.78$. We assume that the evaluations of $f$ to are perturbed by zero-mean Gaussian noise with standard deviation 0.25. The Forrester function is part of the benchmark of functions of GPyOpt. To create the true function, the perturbed version and the boundaries of the problem you need to run the following cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Objective.forrester()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To plot the true $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0.0, 1.0, num=1000)\n",
    "plt.plot(x, obj(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define a GP prior\n",
    "\n",
    "We use GPy package as it has GP training and prediction available for us.\n",
    "\n",
    "We define a simple GP with Square Exponential Kernel \n",
    "\n",
    "GPy models need initial data to define a model so first let's collect few samples from our objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_points(objective, n=3, seed=1234):\n",
    "    np.random.seed(seed=seed)\n",
    "    a, b = objective.limits\n",
    "    scale = b-a\n",
    "    x_init = scale * np.random.rand(n,1) - a\n",
    "\n",
    "    y_init = objective(x_init)\n",
    "    \n",
    "    return x_init, y_init\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Forrester Objective\n",
    "\n",
    "Note in addition to the objective function the class below also has limits or bounds over which function is defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Objective.forrester()\n",
    "obj.limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_0, y_0 = init_points(obj, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a GP model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = GPy.kern.RBF(1)\n",
    "gp_model = GPy.models.GPRegression(x_0, y_0, kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Now train the model and plot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gp_model.optimize()\n",
    "gp_model.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquisition Functions\n",
    "\n",
    "Once we have a GP model we need a method to find the best point to evaluate next. Aquisition functions are used to pick the evaluation points. \n",
    "\n",
    "First we create a Aqusition base class \n",
    "\n",
    "refer lecture notes [slide 17] for the details. \n",
    "\n",
    "\n",
    "For 2 Acquisition functions we need the gamma function: \n",
    "            $$ \\gamma(x) = \\frac{f(x_{\\text{best}}) - \\mu(x) - \\xi }{ \\sigma(x)}$$\n",
    "            \n",
    "           \n",
    "And a mechanism to find the argmax of the acquisition functions [Simplest way is to just evaluate function at a very fine grid and pick the best candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AquisitionBase:\n",
    "    \n",
    "    def __init__(self, Xi=0.0):\n",
    "        \"\"\"\n",
    "        Xi is scalar slack variable, Xi=0.0 pure exploitation \n",
    "        larger values promote explorations [see lecture slides]\n",
    "        \"\"\"\n",
    "        self.Xi = Xi \n",
    "        \n",
    "    \n",
    "    def __call__(model: GPy.Model, x: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        \n",
    "        :param model: GPy regression model [used to get \\mu(x), var(x) = model.predict(x)]\n",
    "        :param x: input at whih we evaluate acquisition function \n",
    "        :return: shape (N, ) or (N, 1)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "    def gamma(self, y_best, mean_x, sigma_x):\n",
    "        \"\"\"\n",
    "        :param y_best: float scalar best value so far \n",
    "        :param mean_x: numpy array of N x D where D is dimension [1 or None in this tutorial]\n",
    "        :param sigma_x:numpy array of N x 1  \n",
    "        :return: shape (N, ) or (N, 1)\n",
    "        \"\"\"\n",
    "        \n",
    "        gamma_x =  0.0 # Edit this line\n",
    "        \n",
    "        return gamma_x\n",
    "\n",
    "    def maximise(self, model, lims):\n",
    "        a, b = lims\n",
    "        x = np.linspace(a, b, num=10000)\n",
    "        y = self.__call__(model, x.reshape(-1, 1))\n",
    "        index = np.argmax(y)\n",
    "        print()\n",
    "        return x[index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Probability of Improvement\n",
    "Complete the following cell with \n",
    "        $$ \\alpha_{\\text{PI}} = \\mathbf(\\Phi) (\\gamma(x)) $$\n",
    "         \n",
    "where $ \\mathbf(\\Phi) $ is CDF of standard Normal distribution\n",
    "##### Hint: You can use  'norm' from scipy.stats instead of implementing CDF by yourself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "class ProbabilityImprovement(AquisitionBase):\n",
    "    \n",
    "    def __call__(self, model, x):\n",
    "        \n",
    "        assert isinstance(model, GPy.Model)\n",
    "        \n",
    "        mean_x = None # Edit This line        \n",
    "        sigma_x = None # Edit This line\n",
    "        \n",
    "        y_best = np.min(model.Y)\n",
    "        \n",
    "        gamma_x = self.gamma(y_best, mean_x, sigma_x) \n",
    "        PI_x = 0.0 # Edit This line\n",
    "        return PI_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### Algorithm  Basic pseudo-code for Bayesian optimization\n",
    "1. Place a Gaussian process prior on $f$\n",
    "2. Observe $f$ at $n_0$ points according to an initial space-filling experimental design.  Set $n=n_0 $.\n",
    "3. __while__ $n≤N$ __do__ \n",
    "   1. Update the posterior probability distribution on $f$ using all available data\n",
    "   - Let $x_n$ be a maximizer of the acquisition function over $x$, where the acquisition function is computed usingthe current posterior distribution.\n",
    "   - Observe $y_n=f(x_n)$.\n",
    "   - Increment $n$\n",
    "   \n",
    "4. __end__ __while__\n",
    "5. Return a solution:  either the point evaluated with the largest $f(x)$, or the point with the largest posteriormean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now complete the optimise class of the class below:\n",
    "\n",
    "the initialisation of class covers the steps 1-3A. Implement steps 3B and 3C\n",
    "\n",
    "#### Hint: these are simple one liners "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianOptmisation:\n",
    "    \n",
    "    def __init__(self, objective, aquisition_function, init_steps=2, kernel=GPy.kern.RBF(1),  seed=1234 ):\n",
    "        \n",
    "        self.objective = objective \n",
    "        x_init, y_init = self.init_points(n=init_steps, seed=seed)\n",
    "        self.aquisition = aquisition_function\n",
    "        self.model = GPy.models.GPRegression(x_init, y_init, kernel)\n",
    "        self.model.optimize()\n",
    "        # GP model fit may be poor uncomment line below if you find that repeated experimetns start with poor GP fit\n",
    "#         self.model.optimize_restarts(20)\n",
    "    \n",
    "    def optimise(self, n_iter=10, plot=False, verbose=True):\n",
    "        \n",
    "        \n",
    "        for i in range(n_iter):\n",
    "            \n",
    "            # Maximise your aquisition function to get next best x\n",
    "            x_n = 0.0  # Edit This line\n",
    "            \n",
    "            # Evaluate objective at best X calculated above\n",
    "            y_n = 0.0 # Edit This line\n",
    "            \n",
    "            # Update your model \n",
    "            self.update_model(np.atleast_2d(x_n), np.atleast_1d(y_n))\n",
    "            \n",
    "            if verbose:\n",
    "                print(f\"Iter: {len(self.model.Y)}, X_best={x_n}, Objective={y_n}\")\n",
    "        \n",
    "            if plot:\n",
    "                self.plot()\n",
    "                plt.show()\n",
    "        \n",
    "    ## Do Not Change anything below [free to experiment but things may break down]    \n",
    "    \n",
    "    def init_points(self, n=2, seed=None):\n",
    "        \n",
    "        if seed is not None:\n",
    "            np.random.seed(seed=seed)\n",
    "            \n",
    "        a, b = self.objective.limits\n",
    "        scale = b-a\n",
    "        x_init = scale * np.random.rand(n,1) + a\n",
    "\n",
    "        y_init = self.objective(x_init)\n",
    "    \n",
    "        return x_init, y_init\n",
    "    \n",
    "    def _get_grid(self, num=100):\n",
    "        a, b = self.objective.limits\n",
    "        x_tb = np.linspace(a, b, num=num)\n",
    "        return x_tb\n",
    "    \n",
    "    def _add_data_points(self, x_in, y_in):\n",
    "        x, y = self.model.X, self.model.Y\n",
    "        new_x = np.vstack((x, x_in))\n",
    "        new_y = np.vstack((y, y_in))\n",
    "        self.model.set_XY(X=new_x, Y=new_y)\n",
    "        \n",
    "    def update_model(self, x_in, y_in):\n",
    "        self._add_data_points(x_in, y_in)\n",
    "        self.model.optimize()\n",
    "               \n",
    "    \n",
    "    def evaluate_objective(self, x_in):\n",
    "        return self.objective(x_in)\n",
    "    \n",
    "    def plot_gp(self, ax=None, gp_model=None):\n",
    "        if gp_model is None:\n",
    "            gp_model = self.model\n",
    "        a, b = self.objective.limits\n",
    "        x_tb = self._get_grid()\n",
    "        x_2d = x_tb.reshape(-1, 1)\n",
    "        mean_x, sigma_x = gp_model.predict(x_2d)\n",
    "        target = self.objective(x_tb)\n",
    "\n",
    "        y1 = mean_x + 1.96 * np.sqrt(sigma_x)\n",
    "        y2 = mean_x - 1.96 * np.sqrt(sigma_x)\n",
    "\n",
    "        if ax is None:\n",
    "            ax = plt.subplot()\n",
    "        ax.plot(x_tb, target, 'r-', label='Objective', linewidth=2.5)\n",
    "        ax.plot(x_tb, mean_x, 'k--', label='Mean')\n",
    "        ax.fill_between(x_tb, y1.flatten(), y2.flatten(), alpha=0.45, label='confidence interval')\n",
    "        ax.scatter(gp_model.X, gp_model.Y, marker='D', label='Data')\n",
    "        ax.set_ylabel('f(x)', fontdict={'size':10})\n",
    "        ax.set_xlabel('x', fontdict={'size':10})\n",
    "        ax.legend()\n",
    "    \n",
    "    def plot_aquisition(self, ax, aquisition=None):\n",
    "        \n",
    "        if aquisition is None:\n",
    "            aquisition = self.aquisition\n",
    "            \n",
    "        x_tb = self._get_grid()\n",
    "        x_2d = x_tb.reshape(-1, 1)\n",
    "        aqui_vals = aquisition(self.model, x_2d)\n",
    "        \n",
    "        if ax is None:\n",
    "            ax = plt.subplot()\n",
    "        ax.plot(x_tb, aqui_vals, label='Acq_fun')\n",
    "        ax.set_ylabel('Acquisition', fontdict={'size':10})\n",
    "        ax.set_xlabel('x', fontdict={'size':10})\n",
    "        ax.legend()\n",
    "    \n",
    "    def plot_objective(self, ax):\n",
    "        x = self._get_grid()\n",
    "        ax.plot(x, self.objective(x), 'r', label='True', linewidth=3)\n",
    "    \n",
    "    def plot(self):\n",
    "        \"\"\"\n",
    "        Helper function for plotting results\n",
    "        \"\"\"\n",
    "                \n",
    "        gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1]) \n",
    "        gp_axis = plt.subplot(gs[0])\n",
    "        acq_axis = plt.subplot(gs[1])\n",
    "        \n",
    "        self.plot_gp(gp_axis)\n",
    "\n",
    "        self.plot_aquisition(acq_axis)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create an Objective\n",
    "Now lets create Bayesian Optimisation object \n",
    "with forrester objective and Probability of Improvement(PI) as acquisition function \n",
    "As this is a 1-D function we initialise Gp with 2 random samples\n",
    "(In real examples one should use space filling/ Low discrepancy  samples like _Sobol_ See here https://en.wikipedia.org/wiki/Low-discrepancy_sequence for details )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "obj = Objective.forrester()\n",
    "acq = ProbabilityImprovement()\n",
    "bo = BayesianOptmisation(obj, aquisition_function=acq, init_steps=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot GP and Acquisition\n",
    "We now have nice plotting functions available let's see how our Acquisition function and surrogate GP model looks\n",
    "Do not worry if your GP fits looks poor compared to true values, note we only saw two samples so we should ideally fit a straight line!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take one step \n",
    "\n",
    "Assuming you have completed the optimisation part correctly "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.optimise(n_iter=1, plot=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take couple more steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo.optimise(n_iter=2, plot=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More Acquisition Functions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected Improvement (EI)\n",
    "\n",
    "We have already implemented $\\gamma(x)$ for PI and also implemented grid search maximsaisation for acquisition functions, in the following we reuse those methos by subclassing our Aquistion base class similar to PI\n",
    "\n",
    "Expected Improvement:\n",
    "    $$\\alpha_{\\text{EI}}(x) =  \\sigma(x) \\left( \\gamma(x)\\mathbf{\\Phi}(x) + \\mathcal{N}(\\gamma(x) | 0, 1) \\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import norm\n",
    "class ExpectedImprovement(AquisitionBase):\n",
    "    \n",
    "    def __call__(self, model, x):\n",
    "        assert isinstance(model, GPy.Model)\n",
    "        \n",
    "        mean_x = None # Edit This line       \n",
    "        sigma_x = None # Edit This line\n",
    "        \n",
    "        y_best = np.min(model.Y)\n",
    "        \n",
    "        gamma_x = self.gamma(y_best, mean_x, sigma_x)\n",
    "        EI = 0.0 # Edit This line\n",
    "        return EI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lower Confidence Bound (LCB)\n",
    "\n",
    "GP LCB :\n",
    "$$\\alpha_{\\text{LCB}}(x) = - \\left( \\mu(x) - \\kappa \\, \\sigma(x)\\right), \\qquad \\kappa > 0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LowerConfidenceBound(AquisitionBase):\n",
    "    \n",
    "    def __init__(self, kappa=2.56):\n",
    "        super(LowerConfidenceBound, self).__init__()\n",
    "        self.kappa = kappa\n",
    "        \n",
    "    def __call__(self, model, x):\n",
    "        assert isinstance(model, GPy.Model)\n",
    "        mean_x = None # Edit This line       \n",
    "        sigma_x = None # Edit This line\n",
    "        LCB = 0.0 # Edit This line\n",
    "        return LCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with Different Acquisition Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With EI "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Objective.forrester()\n",
    "acq_ei = ExpectedImprovement()\n",
    "bo_ei = BayesianOptmisation(obj, aquisition_function=acq_ei, init_steps=2)\n",
    "bo_ei.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_ei.optimise(n_iter=3, plot=True, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With LCB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Objective.forrester()\n",
    "acq_lcb = LowerConfidenceBound()\n",
    "bo_lcb = BayesianOptmisation(obj, aquisition_function=acq_lcb, init_steps=2)\n",
    "bo_lcb.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_lcb.optimise(n_iter=3, plot=True, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploration vs Exploitation \n",
    "\n",
    "Next we investigate role of slack variable in PI and EI and $\\kappa$ in LCB for exploration\n",
    "\n",
    "Forrester function we tried before doesn't have nearby local minimas where optimisation may get stuck. \n",
    "We create a custom cost function with many local minima that are close to true global minimum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj_hump = Objective.humps()\n",
    "obj_hump.noise_std=0.01\n",
    "a, b = obj_hump.limits\n",
    "x_t = np.linspace(a, b, num=100)\n",
    "plt.plot(x_t, obj_hump(x_t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Objective.humps()\n",
    "acq_ei = ExpectedImprovement()\n",
    "bo_ei = BayesianOptmisation(obj, aquisition_function=acq_ei, init_steps=2)\n",
    "bo_ei.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_ei.optimise(n_iter=10, plot=False)\n",
    "bo_ei.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Objective.humps()\n",
    "acq_ei_slack = ExpectedImprovement(Xi=0.1)\n",
    "bo_ei_slack = BayesianOptmisation(obj, aquisition_function=acq_ei_slack, init_steps=2)\n",
    "bo_ei_slack.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_ei_slack.optimise(n_iter=10)\n",
    "bo_ei_slack.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More experiments!!\n",
    "\n",
    "Now run the same experiment, i.e., humps() objective with PI acquisition and LCB [$\\kappa=[0.1, 2.5]$] \n",
    "\n",
    "effect of slack variables is more pronounced on PI acquisition, try different values of slack to see it for yourself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Even More experiments!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Up until now we used Squared exponential Kernel for GP models but we can use different kernels (or their combination) to make better priors depending on the thype of function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obj = Objective.humps()\n",
    "acq_ei = ExpectedImprovement()\n",
    "kernel = GPy.kern.Matern52(1)\n",
    "bo_ei_mattern = BayesianOptmisation(obj, aquisition_function=acq_ei, init_steps=2, kernel=kernel)\n",
    "bo_ei_mattern.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bo_ei_mattern.optimise(n_iter=30)\n",
    "bo_ei_mattern.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
